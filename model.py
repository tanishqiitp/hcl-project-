# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17cxwHNip1vxMV0gEcFC4lpd3-Kxdzc-K

# **IMPORTS**
"""

import pandas as pd
import numpy as np
from datetime import datetime
import random

# ================= STORES =================
stores = pd.DataFrame({
    "store_id": [f"S{i}" for i in range(1, 101)],
    "store_name": [f"Store_{i}" for i in range(1, 101)],
    "store_city": np.random.choice(
        ["Hyderabad", "Bangalore", "Chennai", "Pune", "Delhi"], 100
    ),
    "store_region": np.random.choice(
        ["North", "South", "East", "West"], 100
    ),
    "opening_date": pd.to_datetime(
        np.random.choice(
            ["2018-01-01", "2019-06-01", "2020-01-01", "2021-01-01"], 100
        )
    )
})

# Inject errors
stores.loc[100] = stores.loc[2]               # Duplicate store_id
stores.loc[3, "store_region"] = "INVALID"     # Invalid region
stores.loc[5, "store_name"] = None            # Missing name
stores.loc[7, "opening_date"] = pd.NaT        # Invalid date

stores["opening_date"] = pd.to_datetime(stores["opening_date"], errors="coerce")

# ================= PRODUCTS =================
products = pd.DataFrame({
    "product_id": [f"P{i}" for i in range(1, 101)],
    "product_name": [f"Product_{i}" for i in range(1, 101)],
    "product_category": np.random.choice(
        ["Electronics", "Apparel", "Grocery"], 100
    ),
    "unit_price": np.round(np.random.uniform(50, 3000, 100), 2),
    "current_stock_level": np.random.randint(10, 1000, 100)
})

# Inject errors
products.loc[100] = products.loc[4]           # Duplicate product_id
products.loc[6, "product_category"] = "INVALID"
products.loc[8, "unit_price"] = None
products.loc[10, "current_stock_level"] = -5

# ================= CUSTOMERS =================
customers = pd.DataFrame({
    "customer_id": [f"C{i}" for i in range(1, 101)],
    "first_name": [f"Cust_{i}" for i in range(1, 101)],
    "email": [f"cust{i}@mail.com" for i in range(1, 101)],
    "loyalty_status": np.random.choice(
        ["Bronze", "Silver", "Gold", "VIP"], 100
    ),
    "total_loyalty_points": np.random.randint(100, 5000, 100),
    "last_purchase_date": pd.to_datetime("today") -
        pd.to_timedelta(np.random.randint(1, 365, 100), unit="D"),
    "segment_id": np.random.choice(["HS", "AR", "LS"], 100)
})

# Inject errors
customers.loc[100] = customers.loc[9]          # Duplicate customer_id
customers.loc[11, "email"] = customers.loc[10, "email"]
customers.loc[15, "total_loyalty_points"] = -200
customers.loc[18, "last_purchase_date"] = pd.NaT

customers["last_purchase_date"] = pd.to_datetime(
    customers["last_purchase_date"], errors="coerce"
)

# ================= LOYALTY RULES =================
loyalty_rules = pd.DataFrame({
    "rule_id": [1, 2, 3, 4],
    "rule_name": [
        "Standard Earning",
        "High Value Bonus",
        "Weekend Bonus",
        "Special Campaign"
    ],
    "points_per_unit_spend": [1.0, 1.5, 2.0, -1.0],
    "min_spend_threshold": [0, 2000, 1000, 500],
    "bonus_points": [0, 200, 100, 50]
})

# ================= PROMOTIONS =================
promotions = pd.DataFrame({
    "promotion_id": [f"PR{i}" for i in range(1, 101)],
    "promotion_name": [f"Promo_{i}" for i in range(1, 101)],
    "start_date": pd.date_range("2024-01-01", periods=100, freq="7D"),
    "end_date": pd.date_range("2024-01-10", periods=100, freq="7D"),
    "discount_percentage": np.random.uniform(0.05, 0.5, 100),
    "applicable_category": np.random.choice(
        ["Electronics", "Apparel", "Grocery", "ALL"], 100
    )
})

# Inject errors
promotions.loc[3, "end_date"] = promotions.loc[3, "start_date"] - pd.Timedelta(days=1)
promotions.loc[5, "discount_percentage"] = -0.1
promotions.loc[7, "applicable_category"] = "INVALID"

# ================= SALES HEADER =================
sales_header = pd.DataFrame({
    "transaction_id": [f"T{i}" for i in range(1, 101)],
    "customer_id": np.random.choice(customers["customer_id"], 100),
    "store_id": np.random.choice(stores["store_id"], 100),
    "transaction_date": datetime.today() -
        pd.to_timedelta(np.random.randint(1, 30, 100), unit="D"),
    "total_amount": np.round(np.random.uniform(100, 10000, 100), 2)
})

# Inject errors
sales_header.loc[100] = sales_header.loc[9]
sales_header.loc[12, "customer_id"] = "C999"
sales_header.loc[14, "store_id"] = "INVALID_STORE"
sales_header.loc[16, "total_amount"] = -500

# ================= SALES LINE ITEMS =================
sales_line_items = pd.DataFrame({
    "line_item_id": range(1, 101),
    "transaction_id": np.random.choice(sales_header["transaction_id"], 100),
    "product_id": np.random.choice(products["product_id"], 100),
    "promotion_id": np.random.choice(promotions["promotion_id"], 100),
    "quantity": np.random.randint(1, 5, 100),
    "line_item_amount": np.round(np.random.uniform(100, 5000, 100), 2)
})

# Inject errors
sales_line_items.loc[5, "product_id"] = "P999"
sales_line_items.loc[7, "transaction_id"] = "T999"
sales_line_items.loc[9, "quantity"] = -3
sales_line_items.loc[11, "promotion_id"] = "PR999"

# ================= SAVE FILES =================
stores.to_csv("stores.csv", index=False)
products.to_csv("products.csv", index=False)
customers.to_csv("customers.csv", index=False)
loyalty_rules.to_csv("loyalty_rules.csv", index=False)
promotions.to_csv("promotions.csv", index=False)
sales_header.to_csv("sales_header.csv", index=False)
sales_line_items.to_csv("sales_line_items.csv", index=False)

"""# **case-1**"""

# ================== CLEAN STORES ==================
stores["error_reason"] = ""

# Duplicate store_id
stores.loc[stores.duplicated(subset=["store_id"], keep=False),
           "error_reason"] += "Duplicate store_id | "

# Missing store_name
stores.loc[stores["store_name"].isna(),
           "error_reason"] += "Missing store_name | "

# Invalid store_region
valid_regions = ["North", "South", "East", "West"]
stores.loc[~stores["store_region"].isin(valid_regions),
           "error_reason"] += "Invalid store_region | "

# Invalid opening_date
stores["opening_date"] = pd.to_datetime(stores["opening_date"], errors="coerce")
stores.loc[stores["opening_date"].isna(),
           "error_reason"] += "Invalid opening_date | "

# Opening date in future
stores.loc[stores["opening_date"] > pd.Timestamp.today(),
           "error_reason"] += "Opening date in future | "

staging_stores = stores[stores["error_reason"] == ""].drop(columns=["error_reason"])
quarantine_stores = stores[stores["error_reason"] != ""]

# ================== CLEAN PRODUCTS ==================
products["error_reason"] = ""

# Duplicate product_id
products.loc[products.duplicated(subset=["product_id"], keep=False),
             "error_reason"] += "Duplicate product_id | "

# Missing product_name
products.loc[products["product_name"].isna(),
             "error_reason"] += "Missing product_name | "

# Invalid product_category
valid_categories = ["Electronics", "Apparel", "Grocery"]
products.loc[~products["product_category"].isin(valid_categories),
             "error_reason"] += "Invalid product_category | "

# Invalid unit_price (null or <= 0)
products.loc[products["unit_price"].isna() | (products["unit_price"] <= 0),
             "error_reason"] += "Invalid unit_price | "

# Invalid stock level
products.loc[products["current_stock_level"] < 0,
             "error_reason"] += "Invalid stock_level | "

staging_products = products[products["error_reason"] == ""].drop(columns=["error_reason"])
quarantine_products = products[products["error_reason"] != ""]

# ================== CLEAN CUSTOMERS ==================
customers["error_reason"] = ""

# Duplicate customer_id
customers.loc[customers.duplicated(subset=["customer_id"], keep=False),
              "error_reason"] += "Duplicate customer_id | "

# Duplicate email
customers.loc[customers.duplicated(subset=["email"], keep=False),
              "error_reason"] += "Duplicate email | "

# Invalid email format
customers.loc[~customers["email"].str.contains("@", na=False),
              "error_reason"] += "Invalid email format | "

# Negative loyalty points
customers.loc[customers["total_loyalty_points"] < 0,
              "error_reason"] += "Negative loyalty points | "

# Invalid last_purchase_date
customers["last_purchase_date"] = pd.to_datetime(customers["last_purchase_date"], errors="coerce")
customers.loc[customers["last_purchase_date"].isna(),
              "error_reason"] += "Invalid last_purchase_date | "

staging_customers = customers[customers["error_reason"] == ""].drop(columns=["error_reason"])
quarantine_customers = customers[customers["error_reason"] != ""]

# ================== CLEAN LOYALTY RULES ==================
loyalty_rules["error_reason"] = ""

# Invalid points_per_unit_spend
loyalty_rules.loc[loyalty_rules["points_per_unit_spend"] <= 0,
                  "error_reason"] += "Invalid points_per_unit_spend | "

# Negative min_spend_threshold
loyalty_rules.loc[loyalty_rules["min_spend_threshold"] < 0,
                  "error_reason"] += "Negative min_spend_threshold | "

# Negative bonus_points
loyalty_rules.loc[loyalty_rules["bonus_points"] < 0,
                  "error_reason"] += "Negative bonus_points | "

staging_loyalty_rules = loyalty_rules[loyalty_rules["error_reason"] == ""].drop(columns=["error_reason"])
quarantine_loyalty_rules = loyalty_rules[loyalty_rules["error_reason"] != ""]

# ================== CLEAN PROMOTIONS ==================
promotions["error_reason"] = ""

promotions["start_date"] = pd.to_datetime(promotions["start_date"], errors="coerce")
promotions["end_date"] = pd.to_datetime(promotions["end_date"], errors="coerce")

# Invalid dates
promotions.loc[promotions["start_date"].isna() | promotions["end_date"].isna(),
               "error_reason"] += "Invalid promotion date | "

# Start date after end date
promotions.loc[promotions["start_date"] > promotions["end_date"],
               "error_reason"] += "Start date after end date | "

# Invalid discount percentage
promotions.loc[(promotions["discount_percentage"] <= 0),
               "error_reason"] += "Invalid discount_percentage | "

# Invalid applicable_category
valid_promo_categories = ["Electronics", "Apparel", "Grocery", "ALL"]
promotions.loc[~promotions["applicable_category"].isin(valid_promo_categories),
               "error_reason"] += "Invalid applicable_category | "

staging_promotions = promotions[promotions["error_reason"] == ""].drop(columns=["error_reason"])
quarantine_promotions = promotions[promotions["error_reason"] != ""]

# ================== CLEAN SALES HEADER ==================
sales_header["error_reason"] = ""

# Invalid store_id
sales_header.loc[~sales_header["store_id"].isin(staging_stores["store_id"]),
                 "error_reason"] += "Invalid store_id | "

# Invalid customer_id
sales_header.loc[~sales_header["customer_id"].isin(staging_customers["customer_id"]),
                 "error_reason"] += "Invalid customer_id | "

# Invalid transaction_date
sales_header["transaction_date"] = pd.to_datetime(sales_header["transaction_date"], errors="coerce")
sales_header.loc[sales_header["transaction_date"].isna(),
                 "error_reason"] += "Invalid transaction_date | "

# Negative or zero total_amount
sales_header.loc[sales_header["total_amount"] <= 0,
                 "error_reason"] += "Invalid total_amount | "

staging_sales_header = sales_header[sales_header["error_reason"] == ""].drop(columns=["error_reason"])
quarantine_sales_header = sales_header[sales_header["error_reason"] != ""]

# ================== CLEAN SALES LINE ITEMS ==================
sales_line_items["error_reason"] = ""

# Invalid product_id
sales_line_items.loc[~sales_line_items["product_id"].isin(staging_products["product_id"]),
                     "error_reason"] += "Invalid product_id | "

# Invalid transaction_id
sales_line_items.loc[~sales_line_items["transaction_id"].isin(staging_sales_header["transaction_id"]),
                     "error_reason"] += "Invalid transaction_id | "

# Invalid quantity
sales_line_items.loc[sales_line_items["quantity"] <= 0,
                     "error_reason"] += "Invalid quantity | "

# Invalid promotion_id (if present)
sales_line_items.loc[
    sales_line_items["promotion_id"].notna() &
    ~sales_line_items["promotion_id"].isin(staging_promotions["promotion_id"]),
    "error_reason"
] += "Invalid promotion_id | "

staging_sales_line_items = sales_line_items[sales_line_items["error_reason"] == ""].drop(columns=["error_reason"])
quarantine_sales_line_items = sales_line_items[sales_line_items["error_reason"] != ""]

# ================== SAVE OUTPUTS ==================
staging_stores.to_csv("staging_stores.csv", index=False)
quarantine_stores.to_csv("quarantine_stores.csv", index=False)

staging_products.to_csv("staging_products.csv", index=False)
quarantine_products.to_csv("quarantine_products.csv", index=False)

staging_customers.to_csv("staging_customers.csv", index=False)
quarantine_customers.to_csv("quarantine_customers.csv", index=False)

staging_loyalty_rules.to_csv("staging_loyalty_rules.csv", index=False)
quarantine_loyalty_rules.to_csv("quarantine_loyalty_rules.csv", index=False)

staging_promotions.to_csv("staging_promotions.csv", index=False)
quarantine_promotions.to_csv("quarantine_promotions.csv", index=False)

staging_sales_header.to_csv("staging_sales_header.csv", index=False)
quarantine_sales_header.to_csv("quarantine_sales_header.csv", index=False)

staging_sales_line_items.to_csv("staging_sales_line_items.csv", index=False)
quarantine_sales_line_items.to_csv("quarantine_sales_line_items.csv", index=False)

"""case-2"""

staging_products = pd.read_csv("/content/staging_products.csv")
staging_products.head()

staging_promotions = pd.read_csv("/content/staging_promotions.csv")
staging_promotions.head()

staging_store_sales_line_items= pd.read_csv("/content/staging_sales_line_items.csv")
staging_store_sales_line_items.head()

sales_promo = (
    sales_line_items
    .drop(columns=["error_reason"], errors="ignore")
    .merge(
        staging_sales_header[
            ["transaction_id", "transaction_date", "store_id"]
        ],
        on="transaction_id",
        how="left"
    )
    .merge(
        promotions.drop(columns=["error_reason"], errors="ignore"),
        on="promotion_id",
        how="left"
    )
    .merge(
        products[["product_id", "product_category"]],
        on="product_id",
        how="left"
    )
)

sales_promo["start_date"] = pd.to_datetime(sales_promo["start_date"], errors="coerce")
sales_promo["end_date"] = pd.to_datetime(sales_promo["end_date"], errors="coerce")

sales_promo["is_promo_sale"] = (
    sales_promo["promotion_id"].notna() &
    sales_promo["start_date"].notna() &
    (sales_promo["transaction_date"] >= sales_promo["start_date"]) &
    (sales_promo["transaction_date"] <= sales_promo["end_date"])
)

sales_promo["revenue"] = sales_promo["line_item_amount"] * sales_promo["quantity"]

promoted_sales = sales_promo[sales_promo["is_promo_sale"]].copy()
baseline_sales = sales_promo[~sales_promo["is_promo_sale"]].copy()

# Promotional sales summary
promo_summary = (
    promoted_sales
    .groupby(["promotion_id", "product_category"], as_index=False)
    .agg(promo_units=("quantity","sum"), promo_revenue=("revenue","sum"))
)

# Baseline sales summary
baseline_summary = (
    baseline_sales
    .groupby("product_category", as_index=False)
    .agg(base_units=("quantity","sum"), base_revenue=("revenue","sum"))
)

effectiveness = promo_summary.merge(
    baseline_summary,
    on="product_category",
    how="left"
)

effectiveness[["base_units", "base_revenue"]] = effectiveness[["base_units","base_revenue"]].fillna(0)

effectiveness["sales_lift_pct"] = np.where(
    effectiveness["base_units"] > 0,
    ((effectiveness["promo_units"] - effectiveness["base_units"]) / effectiveness["base_units"]) * 100,
    effectiveness["promo_units"] * 100
)

effectiveness["revenue_lift_pct"] = np.where(
    effectiveness["base_revenue"] > 0,
    ((effectiveness["promo_revenue"] - effectiveness["base_revenue"]) / effectiveness["base_revenue"]) * 100,
    effectiveness["promo_revenue"] * 100
)

top_3_promotions = effectiveness.sort_values("sales_lift_pct", ascending=False).head(3)

print("==== PROMOTION EFFECTIVENESS ====")
effectiveness

print("\n==== TOP 3 PROMOTIONS BY SALES LIFT ====")
top_3_promotions

import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")

viz_df = top_3_promotions.copy()

viz_df["promotion_label"] = (
    viz_df["promotion_id"] + " (" + viz_df["product_category"] + ")"
)

plt.figure(figsize=(8, 5))

sns.barplot(
    data=viz_df,
    x="promotion_label",
    y="sales_lift_pct",
    palette="viridis"
)

plt.title("Top 3 Most Effective Promotions by Sales Lift (%)", fontsize=14)
plt.xlabel("Promotion (Product Category)")
plt.ylabel("Sales Lift (%)")

plt.tight_layout()
plt.show()

"""# **case-3**"""

staging_customers = pd.read_csv("/content/staging_customers.csv")
staging_customers.head()

staging_store_sales_headers = pd.read_csv("/content/staging_sales_header.csv")
staging_store_sales_headers.head()

loyalty_rules = pd.read_csv("/content/loyalty_rules.csv")
loyalty_rules.head()

# 1Ô∏è‚É£ Calculate years as customer
staging_customers["years_as_customer"] = (
    pd.Timestamp.now() - pd.to_datetime(staging_customers["last_purchase_date"])
).dt.days // 365


# 2Ô∏è‚É£ Merge transactions with customers
txn_customer = staging_store_sales_headers.merge(
    staging_customers[["customer_id", "total_loyalty_points", "loyalty_status", "years_as_customer"]],
    on="customer_id",
    how="left"
)

# 3Ô∏è‚É£ Define tiered points per spend
def tiered_points(amount):
    if amount < 1000:
        return 1
    elif amount < 5000:
        return 1.5
    else:
        return 2

txn_customer["points_per_unit"] = txn_customer["total_amount"].apply(tiered_points)

# 4Ô∏è‚É£ Loyalty status multiplier
status_multiplier = {"Bronze":1, "Silver":1.2, "Gold":1.5, "Platinum":2}
txn_customer["status_multiplier"] = txn_customer["loyalty_status"].map(status_multiplier)

# 5Ô∏è‚É£ Tenure bonus
def tenure_bonus(years):
    if years < 1:
        return 0
    elif years < 3:
        return 50
    else:
        return 100

txn_customer["tenure_bonus"] = txn_customer["years_as_customer"].apply(tenure_bonus)

# 6Ô∏è‚É£ Calculate earned points
txn_customer["earned_points"] = (
    txn_customer["total_amount"] * txn_customer["points_per_unit"] * txn_customer["status_multiplier"]
) + txn_customer["tenure_bonus"]

# 7Ô∏è‚É£ Aggregate per customer
customer_points_update = txn_customer.groupby("customer_id", as_index=False).agg(
    new_points=("earned_points", "sum")
)

# 8Ô∏è‚É£ Update customer master table
updated_customers = staging_customers.merge(
    customer_points_update,
    on="customer_id",
    how="left"
)

updated_customers["new_points"] = updated_customers["new_points"].fillna(0)
updated_customers["total_loyalty_points"] = (
    updated_customers["total_loyalty_points"] + updated_customers["new_points"]
)
updated_customers.drop(columns=["new_points"], inplace=True)

# Transaction-level points (points accrued for each transaction)
transaction_points = best_rules.groupby(
    ["transaction_id", "customer_id"], as_index=False
).agg(
    points_earned=("earned_points", "sum")
)

transaction_points.head()

staging_customers.head()

updated_customers.head()

"""caase-4"""

# --------------------------------------------------
# 1. LOAD CLEAN (STAGING) DATA
# --------------------------------------------------
staging_customers = pd.read_csv("staging_customers.csv")
staging_sales_header = pd.read_csv("staging_sales_header.csv")

# Ensure transaction_date is datetime
staging_sales_header["transaction_date"] = pd.to_datetime(
    staging_sales_header["transaction_date"], errors="coerce"
)

# --------------------------------------------------
# 2. AGGREGATE SALES DATA PER CUSTOMER (RFM BASE)
# --------------------------------------------------
sales_agg = (
    staging_sales_header
    .groupby("customer_id", as_index=False)
    .agg(
        frequency=("transaction_id", "count"),
        monetary=("total_amount", "sum"),
        last_purchase_date=("transaction_date", "max")
    )
)

# --------------------------------------------------
# 3. CALCULATE RECENCY
# --------------------------------------------------
reference_date = staging_sales_header["transaction_date"].max()

sales_agg["recency"] = (
    reference_date - sales_agg["last_purchase_date"]
).dt.days

# --------------------------------------------------
# 4. MERGE WITH CUSTOMER & LOYALTY DATA
# --------------------------------------------------
rfm = staging_customers.merge(
    sales_agg,
    on="customer_id",
    how="left"
)

# Fill missing values for customers with no transactions
rfm["frequency"] = rfm["frequency"].fillna(0)
rfm["monetary"] = rfm["monetary"].fillna(0)
rfm["recency"] = rfm["recency"].fillna(999)  # Use high number for no purchases
rfm["total_loyalty_points"] = rfm["total_loyalty_points"].fillna(0)

# --------------------------------------------------
# 5. SEGMENT 1: HIGH-SPENDERS (TOP 10% BY MONETARY)
# --------------------------------------------------
high_spender_threshold = rfm["monetary"].quantile(0.90)

rfm["high_spender_segment"] = np.where(
    rfm["monetary"] >= high_spender_threshold,
    "High-Spender",
    "Regular"
)

# --------------------------------------------------
# 6. SEGMENT 2: AT-RISK CUSTOMERS
# No purchase in 60+ days but has loyalty points
# --------------------------------------------------
rfm["at_risk_segment"] = np.where(
    (rfm["recency"] >= 60) & (rfm["total_loyalty_points"] > 0),
    "At-Risk",
    "Active"
)

# --------------------------------------------------
# 7. FINAL CUSTOMER SEGMENTATION VIEW
# --------------------------------------------------
customer_segments = rfm[[
    "customer_id",
    "recency",
    "frequency",
    "monetary",
    "total_loyalty_points",
    "high_spender_segment",
    "at_risk_segment"
]]

# --------------------------------------------------
# 8. SAVE OUTPUT
# --------------------------------------------------
customer_segments.to_csv(
    "customer_segmentation_case_4.csv",
    index=False
)

# --------------------------------------------------
# 9. DISPLAY SAMPLE OUTPUT
# --------------------------------------------------
print("===== CUSTOMER SEGMENTATION (CASE 4) =====")
customer_segments.head(10)

print("\n===== SEGMENT COUNTS =====")
print(customer_segments.groupby(["high_spender_segment", "at_risk_segment"] ).size())

# -------------------------------
# 1. Count of Customers per Segment
# -------------------------------
plt.figure(figsize=(8,5))
sns.countplot(
    data=customer_segments,
    x="high_spender_segment",
    hue="at_risk_segment"
)
plt.title("Customer Segments: High-Spenders vs At-Risk")
plt.ylabel("Number of Customers")
plt.xlabel("High Spender Segment")
plt.legend(title="At-Risk Status")
plt.show()

# -------------------------------
# 2. Distribution of Total Loyalty Points
# -------------------------------
plt.figure(figsize=(8,5))
sns.histplot(
    customer_segments,
    x="total_loyalty_points",
    bins=20,
    hue="high_spender_segment",
    multiple="stack"
)
plt.title("Distribution of Loyalty Points by High-Spender Segment")
plt.xlabel("Total Loyalty Points")
plt.ylabel("Number of Customers")
plt.show()

# -------------------------------
# 3. Scatter: Monetary vs Frequency colored by At-Risk
# -------------------------------
plt.figure(figsize=(8,6))
sns.scatterplot(
    data=customer_segments,
    x="frequency",
    y="monetary",
    hue="at_risk_segment",
    size="total_loyalty_points",
    sizes=(20,200),
    alpha=0.7
)
plt.title("Customer Spend vs Purchase Frequency")
plt.xlabel("Frequency of Purchases")
plt.ylabel("Total Monetary Spend")
plt.legend(title="At-Risk Segment")
plt.show()

"""case-5"""

# customers who earned points in this run
customers_with_new_points = customer_points_update[
    customer_points_update["new_points"] > 0
]

notification_df = customers_with_new_points.merge(
    staging_customers[["customer_id", "first_name", "email", "total_loyalty_points"]],
    on="customer_id",
    how="left"
)

def generate_email(row):
    return f"""
    Hi {row['first_name']},

    üéâ Great news! You've just earned {int(row['new_points'])} loyalty points.

    ‚≠ê Your total loyalty balance is now: {int(row['total_loyalty_points'])} points.

    Thank you for shopping with us.
    Keep earning rewards with every purchase!

    ‚Äî Your Loyalty Team
    """

notification_df["email_content"] = notification_df.apply(generate_email, axis=1)

def send_email(email, content):
    print("====================================")
    print(f"Sending email to: {email}")
    print(content)
    print("Email sent successfully ‚úÖ")
    print("====================================\n")

for _, row in notification_df.iterrows():
    send_email(row["email"], row["email_content"])

"""case-6"""

# Convert transaction_date to datetime
staging_sales_header["transaction_date"] = pd.to_datetime(
    staging_sales_header["transaction_date"], errors="coerce"
)

# Merge sales_line_items with store info from sales_header
sales_items = staging_sales_line_items.merge(
    staging_sales_header[["transaction_id", "store_id", "transaction_date"]],
    on="transaction_id",
    how="left"
)

# --------------------------------------------------
#  SIMULATE DAILY STORE INVENTORY
# --------------------------------------------------
dates = pd.date_range(
    staging_sales_header["transaction_date"].min(),
    staging_sales_header["transaction_date"].max()
)

inventory_records = []
for store in staging_stores["store_id"]:
    for product in staging_products["product_id"]:
        for date in dates:
            inventory_records.append({
                "store_id": store,
                "product_id": product,
                "date": date,
                "stock_level": np.random.randint(0, 50)  # Non-negative stock
            })

inventory = pd.DataFrame(inventory_records)

# --------------------------------------------------
#  IDENTIFY TOP 5 BEST-SELLING PRODUCTS
# --------------------------------------------------
top_products = (
    sales_items.groupby("product_id")
    .agg(total_units=("quantity", "sum"))
    .sort_values("total_units", ascending=False)
    .head(5)
    .reset_index()
)

# --------------------------------------------------
#  FILTER INVENTORY FOR TOP PRODUCTS
# --------------------------------------------------
top_inventory = inventory.merge(top_products, on="product_id", how="inner")

# --------------------------------------------------
#  CALCULATE OUT-OF-STOCK PERCENTAGE
# --------------------------------------------------
stock_analysis = (
    top_inventory
    .groupby(["store_id", "product_id"])
    .agg(
        total_days=("date", "count"),
        out_of_stock_days=("stock_level", lambda x: (x <= 0).sum())
    )
    .reset_index()
)

stock_analysis["out_of_stock_pct"] = (
    stock_analysis["out_of_stock_days"] / stock_analysis["total_days"] * 100
)

# --------------------------------------------------
#  ESTIMATE POTENTIAL LOST SALES
# --------------------------------------------------
# Average daily sales per store-product for top products
avg_sales = (
    sales_items[sales_items["product_id"].isin(top_products["product_id"])]
    .groupby(["store_id", "product_id"], as_index=False)
    .agg(avg_daily_sales=("quantity", "mean"))
)

stock_analysis = stock_analysis.merge(
    avg_sales, on=["store_id", "product_id"], how="left"
).merge(
    staging_products[["product_id", "product_name", "unit_price"]],
    on="product_id",
    how="left"
)

# Fill missing values
stock_analysis["unit_price"] = stock_analysis["unit_price"].fillna(0)
stock_analysis["avg_daily_sales"] = stock_analysis["avg_daily_sales"].fillna(0)

# Calculate estimated lost revenue
stock_analysis["estimated_lost_revenue"] = (
    stock_analysis["out_of_stock_days"]
    * stock_analysis["avg_daily_sales"]
    * stock_analysis["unit_price"]
)

# --------------------------------------------------
#  FINAL OUTPUT
# --------------------------------------------------
final_inventory_impact = stock_analysis.sort_values(
    "estimated_lost_revenue", ascending=False
)

final_inventory_impact.to_csv(
    "inventory_sales_impact_case_6.csv", index=False
)

print("===== INVENTORY & SALES IMPACT (CASE 6) =====")
print(final_inventory_impact.head(10))

plt.figure()
plt.bar(top_products["product_id"], top_products["total_units"])
plt.xlabel("Product ID")
plt.ylabel("Total Units Sold")
plt.title("Top 5 Best-Selling Products")
plt.xticks(rotation=45)
plt.tight_layout()

plt.show()

top_store_product_loss = final_inventory_impact.head(10)

plt.figure()
plt.barh(
    top_store_product_loss["store_id"] + " | " +
    top_store_product_loss["product_name"],
    top_store_product_loss["estimated_lost_revenue"]
)
plt.xlabel("Estimated Lost Revenue")
plt.ylabel("Store | Product")
plt.title("Top 10 Store‚ÄìProduct Loss Contributors")
plt.tight_layout()
plt.show()